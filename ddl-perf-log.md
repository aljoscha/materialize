# DDL Performance Optimization Log

## Current Setup

Problem: individual DDL statements (CREATE TABLE, CREATE VIEW, CREATE INDEX,
DROP, ALTER) become slow when thousands of objects already exist. A statement
that takes milliseconds on a fresh environment can take multiple seconds at
scale. We measure per-statement latency at various object counts.

```bash
# Debug build (faster to compile, makes O(n) scaling more visible)
bin/environmentd --build-only
bin/environmentd -- --system-parameter-default=log_filter=info

# Optimized build (for confirming fixes against realistic absolute numbers)
bin/environmentd --optimized --build-only
bin/environmentd --optimized -- --system-parameter-default=log_filter=info

# Connect as materialize user (external port)
psql -U materialize -h localhost -p 6875 materialize

# Connect as mz_system (internal port — needed for ALTER SYSTEM SET, etc.)
psql -U mz_system -h localhost -p 6877 materialize
```

**Important: Don't use `--reset` between runs.** Creating 50k tables takes a
very long time. Reuse existing state across runs and even when switching between
debug and optimized builds. Instead: start environmentd without `--reset`,
inspect current state (e.g. `SELECT count(*) FROM mz_objects`), verify the
expected number of objects, and proceed. Only use `--reset` if the state is
actually corrupt or you explicitly need a fresh start.

After a fresh `--reset` (or first-ever start), you must raise object limits
before creating large numbers of objects:
```bash
psql -U mz_system -h localhost -p 6877 materialize -c "
  ALTER SYSTEM SET max_tables = 100000;
  ALTER SYSTEM SET max_materialized_views = 100000;
  ALTER SYSTEM SET max_objects_per_schema = 100000;
"
```

To bulk-create objects, write a SQL file and use `psql -f` (much faster than
one psql invocation per statement):
```bash
for i in $(seq 1 1000); do
  echo "CREATE TABLE t_$i (a int, b text);"
done > /tmp/bulk_create.sql
psql -U materialize -h localhost -p 6875 materialize -f /tmp/bulk_create.sql
```

## Profiling Setup

Use `perf record` with frame pointers:

```bash
# Record (after creating objects — run DDL batch during perf recording)
perf record -F 9000 --call-graph fp -p <environmentd-pid> -- sleep <duration>
# (start DDL batch shortly after perf starts)

# Process
perf script | inferno-collapse-perf | rustfilt > /tmp/collapsed.txt

# View as flamegraph
cat /tmp/collapsed.txt | inferno-flamegraph > /tmp/flame.svg

# Or analyze with grep
grep -E 'some_function' /tmp/collapsed.txt | awk '{sum += $NF} END {print sum}'
```

Key profiling notes:
- Use `--call-graph fp` (frame pointers) — the `--profile optimized` build has frame pointers enabled
- 9000 Hz sampling rate gives good resolution without excessive overhead
- Coordinator is single-threaded — DDL goes through the coordinator, so look for
  per-statement CPU time there
- Look for O(n) or O(n^2) patterns: functions whose per-call time grows with object count

## Optimization Log

### Session 1: Baseline measurements (2026-02-18)

**Goal:** Establish baseline DDL latency at increasing object counts.

**Setup:** Debug build, `bin/environmentd` (no `--optimized`). Created tables
with default indexes using `psql -f` batch files. Each table is
`CREATE TABLE bt_N (a int, b text)` which also creates a default index, so
each table contributes ~2 user objects.

**Measurement method:** 5 individual `psql -c` invocations per DDL type at each
scale, measuring wall-clock time with `date +%s%N`. Each sample is a fresh psql
connection → execute DDL → close.

**Results (median of 5 samples, debug build):**

| User Objects | CREATE TABLE | CREATE VIEW | CREATE INDEX | DROP TABLE |
|-------------|-------------|-------------|-------------|------------|
| 0           | 225 ms      | 182 ms      | 218 ms      | 176 ms     |
| ~116        | 250 ms      | 207 ms      | 229 ms      | 193 ms     |
| ~531        | 337 ms      | 287 ms      | 299 ms      | 281 ms     |
| ~1046       | 426 ms      | 374 ms      | 414 ms      | 362 ms     |
| ~2061       | 700 ms      | 630 ms      | 700 ms      | 610 ms     |
| ~3076       | 1034 ms     | 966 ms      | 1006 ms     | 927 ms     |

**Key observations:**
- All DDL types show roughly linear O(n) scaling: ~250-270ms additional latency
  per 1000 user objects.
- All DDL types scale at approximately the same rate, suggesting a shared
  bottleneck in the DDL path rather than type-specific overhead.
- Even the baseline (~225ms for CREATE TABLE) is substantial for a debug build.
  Some of this is likely fixed overhead (persist, cockroach writes, etc).
- At 3000 tables (~6000 user objects counting indexes), a single CREATE TABLE
  takes ~1 second in a debug build.
- The linear scaling across all DDL types points to something in the common DDL
  path that iterates over all existing objects — likely in catalog operations,
  dependency tracking, or coordinator bookkeeping.

**Raw data:** See /tmp/ddl-bench-results.txt for full 5-sample measurements.

**Next step:** Profile the slow path with `perf record` at ~3000 tables to
identify which functions dominate the per-statement cost and where the O(n)
scaling lives.

### Session 2: Profiling — found check_consistency as dominant bottleneck (2026-02-18)

**Goal:** Profile DDL at ~3000 tables to identify where the O(n) scaling lives.

**Setup:** Same debug build. environmentd running with ~3091 user objects (3000+
tables from Session 1). Ran `perf record -F 99 --call-graph fp` during 20
CREATE TABLE statements, processed with `inferno-collapse-perf`.

**Results — Coordinator time breakdown during CREATE TABLE at ~3000 objects:**

| Component | % of coordinator time | Description |
|-----------|----------------------|-------------|
| `check_consistency` | **52.4%** | O(n) consistency check running on EVERY DDL |
| `catalog_transact_inner` (excl check) | 24.5% | Actual catalog mutation + persist writes |
| persist/durable writes | ~17% | Writing to CockroachDB (within transact_inner) |
| `apply_catalog_implications` | 1.5% | Applying side effects (compute/storage) |
| Other coordinator overhead | ~4% | Message handling, sequencing, etc. |

**Inside `check_consistency` breakdown:**

| Sub-check | % of check_consistency | What it does |
|-----------|----------------------|--------------|
| `check_object_dependencies` | **56.1%** | Iterates ALL objects, checks bidirectional dependency consistency |
| `check_items` | **39.7%** | Iterates ALL objects, **re-parses every object's CREATE SQL** |
| `check_internal_fields` | 1.4% | |
| `check_read_holds` | 1.4% | |
| `check_comments` | 0.5% | |
| `check_roles` | 0.4% | |

**Root cause:** `check_consistency()` is called from `catalog_transact_with_side_effects`
and `catalog_transact_with_context` (the two DDL transaction paths) via
`mz_ore::soft_assert_eq_no_log!`. This macro checks `soft_assertions_enabled()`
which returns **true in debug builds** (`cfg!(debug_assertions)`). So every single
DDL statement triggers a full consistency check that iterates over every object in
the catalog.

The two dominant sub-checks are:
1. **`check_object_dependencies`** (56% of check): Iterates all entries in
   `entry_by_id`, for each checks `references()`, `uses()`, `referenced_by()`,
   `used_by()` — verifying bidirectional consistency. O(n × m) where m is avg
   dependencies per object.
2. **`check_items`** (40% of check): Iterates all database → schema → item
   entries and **re-parses the `create_sql` string** for every single object via
   `mz_sql::parse::parse()`. This is the most wasteful — SQL parsing is expensive
   and done for ALL objects on every DDL statement.

**Code locations:**
- `src/adapter/src/coord/ddl.rs:122-126` — soft_assert calling check_consistency
- `src/adapter/src/coord/ddl.rs:189-193` — same in catalog_transact_with_context
- `src/adapter/src/coord/consistency.rs:46` — Coordinator::check_consistency
- `src/adapter/src/catalog/consistency.rs:63` — CatalogState::check_consistency
- `src/ore/src/assert.rs:62` — soft assertions enabled by default in debug builds

**Implications:**
- In **production (release) builds**, `check_consistency` does NOT run (soft
  assertions are disabled), so this bottleneck doesn't affect production.
- In **debug builds** (used for development, testing), this creates severe O(n)
  DDL degradation. This is the entirety of the O(n) scaling we measured in
  Session 1.
- Once we eliminate check_consistency from the hot path, the remaining
  catalog_transact time (~17% persist writes + ~7% in-memory catalog work) may
  reveal a second layer of scaling, but it should be much smaller.

**Next step:** Disable or optimize the consistency check in the DDL hot path.
Options:
1. Skip `check_consistency` entirely during normal DDL in debug builds (only
   run on explicit API call or periodically).
2. Make `check_consistency` incremental — only check the objects that were
   actually modified by the current DDL operation.
3. Optimize the sub-checks: replace `check_items`'s full SQL re-parse with a
   cheaper validation, and optimize `check_object_dependencies`'s contains()
   lookups.

### Session 3: Make check_consistency incremental (2026-02-18)

**Goal:** Eliminate the O(n) scaling from `check_consistency` by making the two
expensive sub-checks (`check_object_dependencies` and `check_items`) only
examine the items affected by the current DDL operation.

**Approach:** Added targeted consistency check methods that scope the expensive
work to only the changed items:
- `check_object_dependencies_for_ids`: only checks dependency bidirectionality
  for the changed items and their immediate dependency neighbors
- `check_items_for_ids`: still iterates schema/item maps (cheap lookups) but
  only re-parses `create_sql` for the changed items
- `check_consistency_for_ids`: replaces the full check in DDL transaction paths

In `coord/ddl.rs`, the two catalog_transact methods now extract affected item
IDs from the ops list before consuming them, then pass those IDs to the targeted
check instead of calling the full `check_consistency()`.

The full `check_consistency()` is preserved for the explicit API endpoint and
other callers — only the DDL hot path uses the incremental version.

**Code changes:**
- `src/adapter/src/catalog/consistency.rs` — added `check_consistency_for_ids`,
  `check_object_dependencies_for_ids`, `check_items_for_ids`
- `src/adapter/src/coord/consistency.rs` — added `check_consistency_for_ids`,
  `affected_item_ids`
- `src/adapter/src/coord/ddl.rs` — modified `catalog_transact_with_side_effects`
  and `catalog_transact_with_context` to use targeted checks

**Results (median of 5 samples, debug build):**

| User Objects | Before (Session 1) | After (Session 3) | Improvement |
|-------------|--------------------|--------------------|-------------|
| 0           | 225 ms             | 183 ms             | 19%         |
| ~116        | 250 ms             | 200 ms             | 20%         |
| ~531        | 337 ms             | 254 ms             | 25%         |
| ~1046       | 426 ms             | 330 ms             | 23%         |
| ~2061       | 700 ms             | 443 ms             | 37%         |
| ~3076       | 1034 ms            | 600 ms             | 42%         |

**Key observations:**
- The O(n) scaling slope improved from ~270ms/1000 objects to ~140ms/1000
  objects — roughly a **2x improvement in scaling**.
- At 3000 tables, CREATE TABLE went from ~1034ms to ~600ms (42% faster).
- The improvement grows with scale (19% at baseline → 42% at 3000 tables),
  confirming we eliminated a significant O(n) component.
- There is still O(n) scaling at ~140ms/1000 objects — this comes from the
  remaining cheap sub-checks (check_internal_fields, check_roles, check_comments
  etc.) which still iterate all objects, plus some O(n) cost in the catalog
  transaction path itself (e.g. builtin table updates, clones, etc.).

**Next step:** Profile again at ~3000 tables to identify the remaining O(n)
sources. The remaining ~140ms/1000 objects is split between the cheap
consistency sub-checks (which could also be made incremental) and the actual
catalog transaction overhead.

### Session 4: Second profiling — identified multiple O(n) layers (2026-02-18)

**Goal:** Profile DDL again after Session 3's incremental check_consistency fix
to identify the remaining O(n) sources behind ~140ms/1000 objects scaling.

**Setup:** Debug build with Session 3 changes applied. environmentd running with
~3091 user objects (~3031 tables). Ran `perf record -F 99 --call-graph fp`
during 30 CREATE TABLE statements (took 17.7s total, ~590ms/DDL — consistent
with Session 3 measurements).

**Results — Full DDL time breakdown (`sequence_create_table` at ~3000 tables):**

| Component | % of DDL time | O(n)? | Description |
|-----------|--------------|-------|-------------|
| **Cow\<CatalogState\>::to_mut** | **16.1%** | **YES** | Clones ENTIRE CatalogState on every DDL |
| **drop(CatalogState clone)** | **15.1%** | **YES** | Drops the full clone after transaction |
| check_consistency_for_ids | 17.6% | partial | Incremental, but sub-checks still iterate all |
| allocate_user_id | 16.9% | YES | Allocates IDs via persist snapshot (see below) |
| with_snapshot (persist trace replay) | 11.5% | YES | Replays ALL trace entries into BTreeMaps |
| TableTransaction::verify | 11.4% | YES | Iterates all items checking uniqueness |
| TableTransaction::new | 6.7% | YES | Converts ALL snapshot entries proto→Rust |
| Persist actual writes | 7.7% | no | O(1) writes for the DDL operation |
| apply_updates | 5.0% | no | In-memory catalog state update |
| apply_catalog_implications | 3.9% | no | Compute/storage side effects |

**Inside `check_consistency_for_ids` (17.6% of total):**

| Sub-check | % of check_consistency | Still O(n)? |
|-----------|----------------------|-------------|
| check_object_dependencies_for_ids | 59.6% | Small n (only changed items + neighbors) |
| check_internal_fields | 14.2% | YES — iterates entry_by_id, entry_by_global_id |
| check_read_holds | 11.6% | Unknown |
| check_items_for_ids | 5.5% | No — only re-parses changed items |
| check_comments | 5.5% | YES — iterates all comments |
| check_roles | 2.8% | YES — iterates entry_by_id |

**Key finding #1: Cow\<CatalogState\> clone+drop = 31.2% of DDL time**

The `transact_inner` function (`src/adapter/src/catalog/transact.rs:576-579`)
creates two `Cow::Borrowed(state)` references:
- `preliminary_state` — tracks intermediate state between ops in the loop
- `state` — the final result

Each calls `.to_mut()` which clones the **entire CatalogState** including:
- `entry_by_id`: BTreeMap\<CatalogItemId, CatalogEntry\> — **13.8% alone** (cloning ~6000 entries)
- `entry_by_global_id`: BTreeMap\<GlobalId, CatalogItemId\> — 0.5%
- All other maps (databases, schemas, clusters, roles, etc.)

After the transaction, both clones are dropped (15.1% of DDL time). Together,
clone+drop accounts for **31.2%** of DDL time and scales O(n) with catalog size.

**Key finding #2: Durable catalog snapshot is rebuilt from scratch every DDL**

The persist-backed durable catalog uses `with_snapshot()` to replay the entire
consolidated trace into BTreeMaps for every transaction. This happens in two
separate paths per CREATE TABLE:
- `allocate_user_id` — allocates new CatalogItemId + GlobalId (14.1% via snapshot)
- `catalog_transact_inner` — the main transaction (11.5% via snapshot)

The `Snapshot` is then wrapped in `TableTransaction` objects:
- `TableTransaction::new` converts all proto entries to Rust types (6.7%)
- `TableTransaction::verify` checks uniqueness of all items (11.4%)

Together, the durable catalog path (with_snapshot + TableTransaction::new +
verify) across both call sites accounts for ~30% of DDL time.

**Summary of O(n) scaling sources:**

| Source | DDL % | Scaling |
|--------|-------|---------|
| Cow\<CatalogState\> clone | 16.1% | O(n) — clones all catalog entries |
| Cow\<CatalogState\> drop | 15.1% | O(n) — drops the clone |
| with_snapshot (2 calls) | 11.5% | O(n) — replays entire persist trace |
| TableTransaction::verify | 11.4% | O(n) — checks all items for uniqueness |
| check_consistency sub-checks | ~6% | O(n) — check_internal_fields, check_roles, etc. |
| TableTransaction::new | 6.7% | O(n) — converts all entries |
| **Total O(n)** | **~67%** | |

**Possible optimizations (ranked by impact):**

1. **Eliminate Cow\<CatalogState\> deep clone (31.2%)** — The biggest single win.
   Options:
   - Use `Arc` fields inside CatalogState so clone is O(1) (clone-on-write at
     field level rather than full struct clone)
   - Restructure `transact_inner` to mutate in place with rollback capability
   - Use persistent/immutable data structures (e.g., `im` crate) for the large
     BTreeMaps

2. **Cache or incrementally update the durable catalog snapshot (~30%)** — Instead
   of replaying the entire trace on every transaction, maintain a cached Snapshot
   and apply only the deltas from the latest transaction. This would make
   `with_snapshot`, `TableTransaction::new`, and `TableTransaction::verify` all
   O(delta) instead of O(n).

3. **Make remaining check_consistency sub-checks incremental (~6%)** — Same
   approach as Session 3 but for `check_internal_fields`, `check_roles`,
   `check_comments`, and `check_read_holds`.

**Next step:** Tackle optimization #1 (Cow clone elimination) as it's the
single largest O(n) source at 31.2% of DDL time. The most promising approach is
to use `Arc` or persistent data structures for the large maps inside
CatalogState so that `Cow::to_mut` becomes O(1) instead of O(n).

### Session 5: Skip preliminary_state clone for last/only op (2026-02-18)

**Goal:** Eliminate one of the two Cow\<CatalogState\> deep clones that happen
on every DDL transaction.

**Analysis:** `transact_inner` (`src/adapter/src/catalog/transact.rs`) creates
two `Cow::Borrowed(state)` references:
- `preliminary_state` — accumulates intermediate state between ops in a loop
- `state` — used for the final consolidated apply

For each op in the loop, `preliminary_state.to_mut().apply_updates()` is called
to make subsequent ops see the effects of earlier ones. But on the **last op**,
there's no subsequent op — the intermediate state is discarded and the final
`state` Cow redoes the consolidated apply. This means the `preliminary_state`
clone is completely wasted for single-op DDL (which is the common case for
individual CREATE TABLE, DROP, etc.).

**Approach:** Skip the `preliminary_state.to_mut().apply_updates()` call on the
last iteration of the ops loop. This avoids the O(n) CatalogState deep clone
when there's only one op (or on the last op of a multi-op transaction).

**Code change:**
- `src/adapter/src/catalog/transact.rs` — changed `for op in ops` to
  `for (op_index, op) in ops.into_iter().enumerate()`, added `is_last_op`
  check to skip the `preliminary_state` apply on the final iteration.

**Results (median of 5 samples, debug build):**

| User Objects | Session 1 (baseline) | Session 3 (incr check) | Session 5 (skip clone) | S5 vs S3 | S5 vs S1 |
|-------------|---------------------|----------------------|----------------------|----------|----------|
| 0           | 225 ms              | 183 ms               | 163 ms               | 11%      | 28%      |
| ~116        | 250 ms              | 200 ms               | 176 ms               | 12%      | 30%      |
| ~531        | 337 ms              | 254 ms               | 226 ms               | 11%      | 33%      |
| ~1046       | 426 ms              | 330 ms               | 280 ms               | 15%      | 34%      |
| ~2061       | 700 ms              | 443 ms               | 399 ms               | 10%      | 43%      |
| ~3076       | 1034 ms             | 600 ms               | 513 ms               | 14%      | 50%      |

**Key observations:**
- ~14% improvement over Session 3 across all scales, consistent with eliminating
  one of the two CatalogState clones (~half of the 31.2% Cow overhead).
- Scaling slope improved from ~136ms to ~114ms per 1000 objects (16% reduction).
- At 3000 tables, CREATE TABLE: 1034ms → 513ms total improvement (**50% faster**
  than original baseline).
- The remaining Cow clone (the `state` Cow) still accounts for ~16% of DDL time.
  Eliminating it would require restructuring `transact_inner` to mutate in place
  or using persistent data structures.

**Next step:** The remaining O(n) sources are:
1. The `state` Cow clone+drop (~16% of DDL time) — harder to eliminate without
   restructuring transact_inner
2. Durable catalog snapshot rebuild (~30%) — with_snapshot, TableTransaction
3. Remaining check_consistency sub-checks (~6%)

The durable catalog snapshot rebuild (~30%) is the next largest target. It
requires making `with_snapshot` incremental or caching the snapshot.

### Session 6: Optimized build profiling — real production bottlenecks (2026-02-19)

**Goal:** Profile DDL with an optimized (`--optimized`) build to identify the
real production bottlenecks, since previous sessions used debug builds where
`check_consistency` (a debug-only assertion) dominated.

**Setup:** Optimized build with `bin/environmentd --reset --optimized`. Created
50,050 tables (50,090 user objects total). Ran `perf record -F 7000 --call-graph
fp` during 30 CREATE TABLE statements.

**Baseline measurements (optimized build, ~50k user objects):**

| DDL Type     | Median (ms) | Notes |
|-------------|-------------|-------|
| CREATE TABLE | 444         | 5 samples: 409, 425, 444, 473, 480 |
| CREATE VIEW  | 436         | 5 samples: 392, 404, 456, 466, 466 |
| DROP TABLE   | 311         | 5 samples: 310, 311, 311, 314, 370 |

**Key finding: No `check_consistency` in production.** In optimized (release)
builds, soft assertions are disabled, so `check_consistency` never runs. All
the time we spent making it incremental (Sessions 3-5) only helps debug/test
builds. The production profile is entirely different.

**Results — Full DDL time breakdown (optimized build, ~50k objects):**

```
A. allocate_user_id path: 24.9%
   - snapshot rebuild from trace: 11.7%
   - Transaction::new (proto→Rust conversion): 8.2%
   - compare_and_append (persist write): 3.6%
   - consolidation: 3.5%

B. catalog_transact_with_side_effects: 75.0%
   - Cow::to_mut (CatalogState clone): 19.4%
   - DurableCatalogState::transaction: 20.0%
     - snapshot rebuild from trace: 12.0%
     - Transaction::new (proto→Rust): 8.2%
   - drop(CatalogState): 10.7%
   - apply_updates: 4.2%
   - apply_catalog_implications: 3.4%
   - persist writes: 4.2%
   - other: ~1%
```

**O(n) vs O(1) classification:**

| Category | DDL % | O(n)? |
|----------|-------|-------|
| Cow\<CatalogState\> clone+drop | 30.1% | YES — clones/drops all catalog entries |
| Snapshot rebuild from trace (2×) | 23.7% | YES — iterates full trace, clones into BTreeMaps |
| Transaction::new proto→Rust (2×) | 16.4% | YES — deserializes all proto entries to Rust types |
| apply_updates | 4.2% | partial |
| consolidation | 3.5% | YES |
| **Total O(n)** | **~78%** | |
| Persist writes (compare_and_append) | ~7.8% | NO — O(1) |
| apply_catalog_implications | 3.4% | NO — O(1) |
| Other | ~10% | mixed |

**Critical architectural insight:** Each CREATE TABLE opens **two** full durable
catalog transactions:

1. `allocate_user_id` — opens a full transaction (snapshot rebuild + proto
   conversion for ALL 20 collections) just to read one ID counter, increment
   it, and commit. This is 24.9% of DDL time.

2. `catalog_transact` — opens another full transaction for the actual DDL
   mutation. This is 75.0% of DDL time.

Both transactions rebuild the `Snapshot` struct from scratch by iterating the
entire in-memory trace (O(n) entries), then `Transaction::new` deserializes
every proto entry to Rust types across all 20 collections. For `allocate_id`,
only the `id_allocator` collection (~5 entries) is actually accessed, but all
50k+ items are needlessly deserialized.

**Code locations:**
- `src/catalog/src/durable/persist.rs:1733-1738` — `transaction()` calls
  `self.snapshot()` which calls `with_snapshot`
- `src/catalog/src/durable/persist.rs:740-850` — `with_snapshot` iterates full
  trace into fresh `Snapshot`
- `src/catalog/src/durable/transaction.rs:117-198` — `Transaction::new`
  deserializes all proto→Rust
- `src/catalog/src/durable.rs:323-340` — `allocate_id` opens full transaction
  for a single counter
- `src/adapter/src/catalog/transact.rs:436-440` — `catalog_transact` opens
  second full transaction

**Comparison with debug build profile (Sessions 2-5):**

| Component | Debug build % | Optimized build % | Notes |
|-----------|--------------|-------------------|-------|
| check_consistency | 52% → 17% (after fix) | 0% | Debug-only |
| Cow clone+drop | 31% | 30% | Same in both |
| Snapshot rebuild | ~12% | 24% | Larger % since no check_consistency |
| Transaction::new | ~7% | 16% | Same |
| Persist writes | ~8% | ~8% | Same |

**Possible optimizations (ranked by impact):**

1. **Cache the `Snapshot` in `PersistHandle` (~24%)** — Instead of rebuilding
   the `Snapshot` from the trace on every `transaction()` call, maintain a
   cached `Snapshot` as a live derived view. When `apply_updates()` is called,
   update the cached Snapshot incrementally. This eliminates the O(n) trace
   iteration on every transaction open. Implementation: add
   `cached_snapshot: Snapshot` field to `PersistHandle`, update it in
   `apply_updates`, return a clone in `with_snapshot`.

2. **Eliminate the separate `allocate_user_id` transaction (~25%)** — The
   `allocate_id` method opens a full transaction just for one counter. Options:
   - Move ID allocation into the main `catalog_transact` using the existing
     `Transaction::allocate_user_item_ids()` method. This requires refactoring
     the sequencer to defer ID assignment until inside transact_inner.
   - Create a lightweight `allocate_id` path that reads the counter directly
     from the trace (like `get_next_id` already does) and commits only the
     counter update, without building a full `Snapshot` or `Transaction`.

3. **Eliminate Cow\<CatalogState\> clone+drop (~30%)** — Use `Arc` fields or
   persistent data structures inside CatalogState so clone is O(1). Or
   restructure `transact_inner` to mutate in place with rollback.

4. **Lazy proto→Rust deserialization in Transaction::new (~16%)** — Instead of
   deserializing all 20 collections upfront, lazily deserialize only the
   collections that are actually accessed during the transaction.

**Next step:** Cache the `Snapshot` in `PersistHandle` (optimization #1). This
is the cleanest single change with ~24% impact. It avoids the O(n) trace
iteration on both the `allocate_user_id` and `catalog_transact` paths.

### Session 7: Cache Snapshot in PersistHandle (2026-02-19)

**Goal:** Eliminate the O(n) trace→Snapshot rebuild that happens twice per DDL
(once in `allocate_user_id`, once in `catalog_transact`), accounting for ~24%
of DDL time in the production (optimized) profile.

**Analysis:** Each time `transaction()` is called, it calls `snapshot()` →
`with_snapshot()` which iterates the entire consolidated trace vector
(`self.snapshot: Vec<(StateUpdateKind, Timestamp, Diff)>`) and rebuilds a
`Snapshot` struct (21 BTreeMaps, one per collection). With 50k tables, this
means iterating 50k+ entries and inserting them into BTreeMaps — twice per DDL.

**Approach:** Maintain a pre-built `Snapshot` as a cached field in
`PersistHandle`, updated incrementally:

1. Added `Snapshot::apply_update(&mut self, kind: &StateUpdateKind, diff: Diff)`
   method that applies a single insert/retract to the appropriate BTreeMap,
   extracting the match logic from the old `with_snapshot` into a reusable
   method.

2. Added `cached_snapshot: Option<Snapshot>` field to `PersistHandle`.

3. Modified `apply_updates` to incrementally maintain the cached snapshot:
   when an update passes through `update_applier.apply_update()` and gets
   pushed to the trace, it's also applied to the cached snapshot (if present).
   For `StateUpdateKindJson` (the unopened catalog), the cache is always `None`
   so this is a no-op.

4. Modified `with_snapshot` to check the cache first: if present, clone and
   return it (O(Snapshot size) for the clone, but avoids the O(n) trace
   iteration + BTreeMap insert). On first call (cache miss), builds from trace
   as before and caches the result.

The key insight: after the first `with_snapshot` call builds the cache, every
subsequent `apply_updates` call (from transaction commits) only applies the
small delta (e.g., 1-3 entries for a single DDL) to the existing cached
Snapshot. The next `with_snapshot` call then gets a cache hit and just clones
the pre-built Snapshot.

**Code changes:**
- `src/catalog/src/durable/objects.rs` — added `Snapshot::apply_update` method
- `src/catalog/src/durable/persist.rs` — added `cached_snapshot` field to
  `PersistHandle`, modified `apply_updates` to maintain cache incrementally,
  modified `with_snapshot` to use cache when available

**Results (median of 10 samples, optimized build, ~50k user objects):**

| DDL Type     | Session 6 Baseline | Session 7 Cached | Improvement |
|-------------|-------------------|------------------|-------------|
| CREATE TABLE | 444 ms            | 387 ms           | **13%**     |
| CREATE VIEW  | 436 ms            | 368 ms           | **16%**     |
| DROP TABLE   | 311 ms            | 295 ms           | **5%**      |

**Key observations:**
- CREATE TABLE and CREATE VIEW improved by 13-16%, consistent with eliminating
  one of the two O(n) snapshot rebuilds per DDL. The first `with_snapshot` call
  (in `allocate_user_id`) still rebuilds from trace on the very first DDL after
  startup, but subsequent calls hit the cache.
- DROP TABLE improved less (5%), possibly because DROP has different snapshot
  access patterns or the snapshot rebuild was a smaller fraction of DROP time.
- The `Snapshot` clone in `with_snapshot` (cache hit path) is still O(n) in
  Snapshot size, but much cheaper than iterating the trace and doing BTreeMap
  insertions: a BTreeMap clone copies the tree structure directly, while
  building from trace does individual insertions that must find their position.
- Note: These measurements have some bimodal variance (some samples ~60ms
  slower than others), likely from CockroachDB background activity. Medians
  should be compared rather than individual samples.

**Remaining O(n) sources in production (optimized build, estimated):**
1. **Cow\<CatalogState\> clone+drop (~30%)** — Still the largest single O(n) cost
2. **Transaction::new proto→Rust conversion (~16%)** — Deserializes all proto
   entries to Rust types across 20 collections for each transaction
3. **Snapshot clone in with_snapshot (~12%)** — The cached Snapshot still needs
   to be cloned for each transaction (clone is cheaper than rebuild, but still
   O(n))
4. **Persist consolidation (~3.5%)** — Consolidating the trace after updates

**Next step:** The next biggest target is either:
- **Eliminate the Cow\<CatalogState\> clone+drop (~30%)** — Use `Arc` fields,
  persistent data structures, or restructure `transact_inner` for in-place
  mutation with rollback.
- **Make Transaction::new lazy (~16%)** — Only deserialize the collections that
  are actually accessed during the transaction (e.g., `allocate_user_id` only
  needs `id_allocator`, not all 20 collections).

### Session 8: Lightweight allocate_id — skip full Transaction for ID allocation (2026-02-19)

**Goal:** Eliminate the expensive full `Transaction` construction from the
`allocate_id` path. In Session 6's profiling, `allocate_user_id` accounted for
24.9% of DDL time despite only needing the `id_allocator` collection (~5
entries). The default `allocate_id` implementation opens a full `Transaction`
which deserializes all 20 catalog collections (50k+ items) to Rust types.

**Analysis:** The `allocate_id` method on the `DurableCatalogState` trait has
a default implementation that:
1. Calls `self.transaction()` — builds full `Snapshot` (21 BTreeMaps, now cached)
   and constructs `Transaction::new` which deserializes all 20 collections from
   proto to Rust types, each with uniqueness verification
2. Calls `txn.get_and_increment_id_by()` — reads one entry from `id_allocator`
3. Calls `txn.commit_internal()` — serializes pending changes back to proto,
   consolidates all 20 (mostly empty) collection diffs, commits to persist

The bottleneck is step 1: even with the cached snapshot (Session 7),
`Transaction::new` still deserializes ~50k items from proto BTreeMaps into Rust
BTreeMaps, and runs uniqueness verification on each. The `allocate_id` path
only touches 1 of the 20 collections, yet pays full O(n) deserialization cost.

**Approach:** Override `allocate_id` in `impl DurableCatalogState for
PersistCatalogState` with a lightweight path:

1. Added `snapshot_id_allocator()` method on `PersistHandle<StateUpdateKind, U>`
   that extracts only the `id_allocator` BTreeMap from the cached snapshot
   (cloning ~5 entries instead of all 21 BTreeMaps). Falls back to scanning the
   trace for `IdAllocator` entries if no cache exists.

2. The override:
   - Calls `snapshot_id_allocator()` to get just the ~5-entry `id_allocator` map
   - Does the get-and-increment logic directly on proto types (no Rust
     deserialization needed)
   - Builds a `TransactionBatch` with all 20 collection diff vectors empty
     except `id_allocator` (which has the 2-entry retract/insert diff)
   - Commits via the existing `commit_transaction` path

This makes `allocate_id` O(1) regardless of catalog size — it never touches
the 50k items, schemas, comments, etc.

**Code changes:**
- `src/catalog/src/durable/persist.rs` — added `snapshot_id_allocator()` method
  on `PersistHandle`, added `allocate_id` override in `impl DurableCatalogState
  for PersistCatalogState`

**Results (median of 7 samples, optimized build, ~50k user objects):**

| DDL Type     | Session 6 (baseline) | Session 7 (cached) | Session 8 (lightweight alloc) | S8 vs S7 | S8 vs S6 |
|-------------|---------------------|-------------------|-------------------------------|----------|----------|
| CREATE TABLE | 444 ms              | 387 ms            | 320 ms                        | **17%**  | **28%**  |
| CREATE VIEW  | 436 ms              | 368 ms            | 295 ms                        | **20%**  | **32%**  |
| DROP TABLE   | 311 ms              | 295 ms            | 282 ms                        | **4%**   | **9%**   |
| DROP VIEW    | —                   | —                 | 278 ms                        | —        | —        |

**allocate_id Prometheus histogram (50k calls during bulk creation):**
- p50: 4-8ms
- p99: 16-32ms
- Average: 8.2ms
- Pre-optimization estimate (from Session 6 profiling): ~110ms

This is a **~14x improvement** in the `allocate_id` path itself.

**Key observations:**
- CREATE TABLE/VIEW improved 17-20% over Session 7, consistent with eliminating
  the O(n) deserialization in `allocate_id` (~16% of total DDL time from
  Transaction::new proto→Rust conversion in the allocate path).
- DROP TABLE/VIEW improved only ~4%, as expected — DROP doesn't need to allocate
  new IDs, so `allocate_id` is not in the DROP path.
- Cumulative improvement from Session 6 baseline: CREATE TABLE 28% faster,
  CREATE VIEW 32% faster.
- The allocate_id path itself went from ~110ms (estimated) to ~8ms average —
  effectively constant time regardless of catalog size.

**Remaining O(n) sources in production (optimized build, estimated):**
1. **Cow\<CatalogState\> clone+drop (~30%)** — Still the largest single O(n) cost
2. **Transaction::new proto→Rust in catalog_transact (~8%)** — The main
   transaction still deserializes all 20 collections; only the allocate_id path
   was optimized
3. **Snapshot clone in with_snapshot (~12%)** — Cached but still O(n) to clone
4. **Persist consolidation (~3.5%)** — Consolidating the trace after updates

**Next step:** The next biggest targets are:
- **Eliminate the Cow\<CatalogState\> clone+drop (~30%)** — Use `Arc` fields,
  persistent data structures, or restructure `transact_inner` for in-place
  mutation with rollback. This is the single largest remaining O(n) cost.
- **Make Transaction::new lazy for catalog_transact (~8%)** — The main
  transaction path still deserializes all 20 collections. Could lazily
  deserialize only accessed collections.

### Session 9: Benchmark persistent CatalogState (imbl::OrdMap + Arc) (2026-02-19)

**Goal:** Benchmark the persistent/immutable data structure change to CatalogState
(commit `b463a07f80`) against the Session 6/8 baselines. This change was designed
to eliminate the ~30% Cow\<CatalogState\> clone+drop cost by making
`CatalogState::clone()` effectively O(1).

**Changes being benchmarked** (commits `5a1d9b3056` and `b463a07f80`):

1. **Replace all BTreeMap fields with `imbl::OrdMap`** — clone is O(1) via
   structural sharing (refcount bump on shared tree root), mutations are O(log n)
   with path-copying. Fields changed: `database_by_name`, `database_by_id`,
   `entry_by_id`, `entry_by_global_id`, `ambient_schemas_by_name`,
   `ambient_schemas_by_id`, `clusters_by_name`, `clusters_by_id`,
   `roles_by_name`, `roles_by_id`, `network_policies_by_name`,
   `network_policies_by_id`, `role_auth_by_id`, `source_references`,
   `temporary_schemas` (15 maps total).

2. **Wrap expensive non-map fields in `Arc`** — clone is O(1) (refcount bump),
   mutation via `Arc::make_mut()` which COW-clones only when shared. Fields:
   `system_configuration`, `default_privileges`, `system_privileges`, `comments`,
   `storage_metadata`.

3. **`MutableMap` trait** — abstraction in apply.rs so helper functions work
   uniformly with both `BTreeMap` (nested inside value types) and `imbl::OrdMap`
   (CatalogState top-level fields).

4. **Prerequisite: replaced `im` crate with `imbl`** — the `im` crate is
   unmaintained; `imbl` is a compatible, maintained fork.

**Setup:** Optimized build (`bin/environmentd --reset --optimized`). Created
50,000 tables (50,001 user objects total). Ran 7 samples × 2 runs per DDL type
(14 total samples per DDL type).

**Results (median of 14 samples, optimized build, ~50k user objects):**

| DDL Type     | Session 6 (baseline) | Session 8 (prev best) | Session 9 (persistent) | S9 vs S8 | S9 vs S6 |
|-------------|---------------------|----------------------|------------------------|----------|----------|
| CREATE TABLE | 444 ms              | 320 ms               | 262 ms                 | **18%**  | **41%**  |
| CREATE VIEW  | 436 ms              | 295 ms               | 228 ms                 | **23%**  | **48%**  |
| DROP TABLE   | 311 ms              | 282 ms               | 221 ms                 | **22%**  | **29%**  |
| DROP VIEW    | —                   | 278 ms               | 211 ms                 | **24%**  | —        |

**Raw data (sorted, 14 samples each):**
- CREATE TABLE: 249 255 255 255 256 260 260 264 284 309 311 314 317 559
- CREATE VIEW:  220 221 221 221 224 225 228 228 230 231 234 234 278 289
- DROP TABLE:   218 218 219 219 220 220 221 222 224 225 274 278 281 287
- DROP VIEW:    207 207 208 208 209 209 210 212 212 213 264 267 273 286

**Key observations:**

- **18-24% improvement over Session 8** across all DDL types. This is consistent
  with eliminating the Cow\<CatalogState\> deep clone+drop, which was estimated
  at ~30% of DDL time. The improvement is less than the full 30% because:
  - The `imbl::OrdMap` mutations (path-copying at O(log n)) have slightly higher
    per-operation cost than BTreeMap mutations, trading single-op speed for O(1)
    clone.
  - The `Cow::to_mut()` clone that was previously O(n) is now O(1), but there
    are still other O(n) costs (Snapshot clone, Transaction::new, etc.).

- **41-48% cumulative improvement over Session 6 baseline** for CREATE operations.
  Combined effect of all optimizations: cached Snapshot (Session 7) +
  lightweight allocate_id (Session 8) + persistent CatalogState (Session 9).

- **DROP operations improved 22-24% over Session 8.** DROPs don't go through
  `allocate_id`, so Session 8 gave them minimal benefit. The persistent data
  structure change helps DROPs equally since the Cow\<CatalogState\> clone+drop
  is in the common DDL transaction path.

- **Bimodal variance observed:** Some samples (~10-15% of them) show spikes
  40-100ms above the cluster, likely from CockroachDB background activity or
  persist consolidation. The median is robust to these outliers.

**Cumulative progress (optimized build, ~50k objects):**

| DDL Type     | Session 6 | Session 7 | Session 8 | Session 9 | Total improvement |
|-------------|-----------|-----------|-----------|-----------|-------------------|
| CREATE TABLE | 444 ms    | 387 ms    | 320 ms    | 262 ms    | **41%**           |
| CREATE VIEW  | 436 ms    | 368 ms    | 295 ms    | 228 ms    | **48%**           |
| DROP TABLE   | 311 ms    | 295 ms    | 282 ms    | 221 ms    | **29%**           |
| DROP VIEW    | —         | —         | 278 ms    | 211 ms    | —                 |

**Remaining O(n) sources in production (estimated post-Session 9):**
1. **Transaction::new proto→Rust in catalog_transact (~8-12%)** — The main DDL
   transaction still deserializes all 20 collections from proto to Rust types.
   Most DDL operations only access a few collections.
2. **Snapshot clone in with_snapshot (~12-15%)** — The cached Snapshot is still
   fully cloned (BTreeMaps) for each transaction. Could use `Arc<BTreeMap>` so
   clone shares data.
3. **Persist consolidation (~3-5%)** — Consolidating the trace after updates.
4. **apply_updates and other bookkeeping (~5%)** — In-memory state updates.

**Next step:** Profile again to confirm the new cost breakdown and identify the
next dominant bottleneck. The two most promising targets are:
- **Make Transaction::new lazy** — only deserialize accessed collections
- **Reduce Snapshot clone cost** — use Arc\<BTreeMap\> for the large maps

### Session 10: Post-Session 9 profiling — group_commit table advancement is new top bottleneck (2026-02-19)

**Goal:** Profile DDL after all Sessions 7-9 optimizations to identify the new
dominant bottleneck now that Cow\<CatalogState\> clone+drop and allocate_id have
been eliminated.

**Setup:** Optimized build (same binary as Session 9). Started with ~10k user
objects (~9720 user tables). Ran `perf record -F 99 --call-graph fp` during 30
CREATE TABLE statements. Processed with `inferno-collapse-perf | rustfilt`.

**DDL latency at ~10k objects (optimized build):**

| DDL Type     | Median (ms) | Notes |
|-------------|-------------|-------|
| CREATE TABLE | 374         | 5 samples: 368, 373, 374, 384, 389 |

Note: Session 9 measured ~262ms at ~50k objects. The higher latency here (~374ms)
may reflect CockroachDB variance, different data directory state, or the `--reset`
startup overhead amortizing. The relative cost breakdown is what matters.

**Results — Full DDL time breakdown (optimized build, ~10k objects):**

Percentages are of `catalog_transact_with_side_effects` time (which is 79.9%
of total coordinator active time). `allocate_user_id` accounts for another 5.5%.

| Component | % of catalog_transact | O(n)? | Description |
|-----------|----------------------|-------|-------------|
| **group_commit (table advancement)** | **23.1%** | **YES** | Iterates ALL catalog entries, advances ALL table frontiers |
| **Transaction::new (proto→Rust)** | **19.6%** | **YES** | Deserializes all 20 collections from proto to Rust types |
| **snapshot clone** | **19.1%** | **YES** | Clones all BTreeMaps from cached Snapshot |
| commit (persist write) | 13.4% | partial | compare_and_append + consolidation |
| apply_updates (in-memory) | 11.9% | YES | sort_unstable on full StateUpdate trace |
| apply_catalog_implications | 8.5% | no | Compute/storage side effects |
| DDL ops (insert_user_item) | 4.5% | no | The actual DDL operation |

**Key finding #1: group_commit table advancement = 23.1% of DDL time**

Every DDL statement triggers `group_commit()` via `BuiltinTableAppend::execute()`
to update system tables (`mz_tables`, `mz_objects`, etc.). Inside `group_commit`,
there is an O(n) loop at `src/adapter/src/coord/appends.rs:512-516`:

```rust
// Add table advancements for all tables.
for table in self.catalog().entries().filter(|entry| entry.is_table()) {
    appends.entry(table.id()).or_default();
}
```

This iterates ALL catalog entries, filters for `is_table()`, and creates an
appends entry for every table — even tables not modified by the current DDL.
The purpose is to advance all table write frontiers to the new timestamp.

Sub-cost breakdown inside group_commit:
- BTreeMap construction/destruction for ~10k entries: **40%** of group_commit
  (building `appends: BTreeMap<CatalogItemId, SmallVec<TableData>>` and then
  dropping it after `into_iter()`)
- imbl::OrdMap traversal of catalog entries: **19%** (`self.catalog().entries()`)
- `latest_global_id()` lookups per table: **6%** (`last_key_value` on
  `RelationVersion → GlobalId` maps)
- `is_table()` filter checks: **5%**

Then `append_table` sends ALL table entries (including empty advancement entries)
to the persist table worker, which does `compare_and_append` per table.

**Key finding #2: apply_updates dominated by sort_unstable consolidation**

`apply_updates` (11.9%) is dominated by `sort_unstable_by` on the full
`Vec<(StateUpdateKind, Timestamp, Diff)>` trace, called from
`differential_dataflow::consolidation::consolidate_updates_slice_slow`.
This sorts the entire trace (O(n log n) in trace size) on every DDL commit.

**Key finding #3: DDL scaling is now very flat**

At ~10k objects, DDL takes ~374ms. Session 9 measured ~262ms at ~50k. The
surprisingly small difference suggests the O(n) per-object marginal cost is
now very low (~2.8ms per 1000 objects). Most of the ~250-370ms is constant
overhead (persist writes, CockroachDB round-trips, message passing).

**Comparison with Session 6 profile (pre-optimization):**

| Component | Session 6 (50k) | Session 10 (~10k) | Change |
|-----------|-----------------|-------------------|--------|
| allocate_user_id | 24.9% | 5.5% | ✓ Fixed in Session 8 |
| Cow\<CatalogState\> clone+drop | 30.1% | ~0% | ✓ Fixed in Session 9 |
| snapshot rebuild from trace | 23.7% | 19.1% (clone from cache) | ✓ Partially fixed in Session 7 |
| Transaction::new proto→Rust | 16.4% | 19.6% | Proportionally larger |
| group_commit | not separately measured | 23.1% | NEW: previously hidden by larger costs |
| apply_updates | 4.2% | 11.9% | Proportionally larger + sort overhead |

**Summary of remaining O(n) sources (optimized build, ranked by impact):**

| Source | DDL % | Scaling |
|--------|-------|---------|
| group_commit table advancement | 23.1% | O(n) — iterates all tables |
| Transaction::new proto→Rust | 19.6% | O(n) — deserializes all 20 collections |
| Snapshot clone | 19.1% | O(n) — clones all BTreeMaps |
| apply_updates sort/consolidation | 11.9% | O(n log n) — sorts full trace |
| **Total O(n)** | **~74%** | |

**Possible optimizations (ranked by impact):**

1. **Eliminate the O(n) table advancement loop in group_commit (~23%)** — The
   biggest single win. Options:
   - Don't advance every table on every DDL — only advance tables that actually
     had writes in this group commit. Use a separate periodic mechanism to
     advance idle table frontiers.
   - Pre-compute and cache the set of table IDs to avoid iterating all entries.
   - Use a single "advance all tables" storage command instead of per-table
     entries in the appends map.

2. **Make Transaction::new lazy (~20%)** — Only deserialize the collections
   actually accessed during the transaction. Most DDLs only touch `items`,
   `schemas`, and `id_allocator` out of 20 collections.

3. **Reduce Snapshot clone cost (~19%)** — Wrap each BTreeMap in the Snapshot
   in `Arc` so clone is O(1) (reference count bump). Mutations use
   `Arc::make_mut()` for COW semantics.

4. **Reduce apply_updates consolidation cost (~12%)** — Instead of sorting the
   full trace on every update, maintain the trace in sorted order (or use a
   data structure that supports efficient incremental consolidation).

**Next step:** Tackle optimization #1 (eliminate the O(n) table advancement
loop in group_commit) as it's the single largest cost at 23% of DDL time and
is a clear O(n) scaling bottleneck.

### Session 11: Remove O(n) table advancement loop from group_commit (2026-02-19)

**Goal:** Eliminate the O(n) table advancement loop in `group_commit()` that
was identified in Session 10 as the new top bottleneck at 23% of DDL time.

**Analysis:** Every `group_commit()` call iterated ALL catalog entries, filtered
for `is_table()`, and added an empty advancement entry for each table to a
BTreeMap — even tables not modified by the current DDL. This ensured all table
write frontiers advanced together. With ~20k+ tables, this created:
- ~20k BTreeMap insertions (and subsequent drop/deallocation)
- ~20k imbl::OrdMap catalog entry lookups
- ~20k `latest_global_id()` calls
- All sent to the storage layer as individual table entries

**Key insight:** The txn-wal protocol makes explicit per-table advancement
unnecessary. When any transaction commits to the txns shard, the logical upper
of ALL registered data shards advances automatically, including those not
involved in the transaction. The empty advancement entries were doing nothing
useful on the storage side.

**Code changes:**
- `src/adapter/src/coord/appends.rs` — removed the O(n) table advancement loop
  (lines 512-519) and the `group_commit_table_advancement_seconds` metric
- `src/adapter/src/metrics.rs` — removed the unused metric field
- `src/storage-controller/src/persist_handles.rs` — removed the early-return
  optimization in `PersistTableWriteWorker::append` that skipped the txn-wal
  commit for empty updates. This ensures periodic group commits (with no actual
  data writes) still commit to the txns shard, advancing logical uppers for
  all registered tables.

**Results (median of 14 samples, optimized build, ~28k objects):**

| DDL Type     | Session 10 baseline | Session 11 | Improvement |
|-------------|--------------------|-----------:|-------------|
| CREATE TABLE | ~374 ms            | **131 ms** | **65%**     |
| CREATE VIEW  | —                  | **96 ms**  | —           |
| DROP TABLE   | —                  | **97 ms**  | —           |
| DROP VIEW    | —                  | **86 ms**  | —           |

**Raw data (sorted, 14 samples each):**
- CREATE TABLE: 126 128 128 130 130 131 133 134 134 136 140 141 150 155
- CREATE VIEW:  93 94 94 95 95 95 95 96 98 99 99 99 100 117
- DROP TABLE:   94 94 95 96 96 96 97 97 98 99 99 100 115 118
- DROP VIEW:    83 84 85 85 86 86 86 86 87 87 89 89 92 119

**Key observations:**

- **65% improvement in CREATE TABLE** over Session 10's baseline. This is
  larger than the expected 23% from Session 10's profiling because:
  - The advancement loop was O(n) and cost grew with object count
  - Removing it also eliminated downstream O(n) costs: the large BTreeMap
    construction/destruction, the O(n) GlobalId lookups, and the O(n) data
    sent to the storage layer
  - With ~28k objects (vs ~10k in Session 10's profiling), the O(n) cost
    was proportionally larger

- **CREATE TABLE at ~131ms with 28k objects** is dramatically better than
  Session 6's baseline of 444ms at 50k objects — a **70% cumulative improvement**
  from all optimizations (Sessions 7-11).

- **All DDL types now under 140ms** at ~28k objects. The remaining time is
  mostly constant overhead (persist writes, proto conversion, snapshot clone).

**Cumulative progress (optimized build):**

| DDL Type     | Session 6 (50k) | Session 9 (50k) | Session 11 (~28k) | Overall |
|-------------|-----------------|-----------------|-------------------|---------|
| CREATE TABLE | 444 ms          | 262 ms          | 131 ms            | **70%** |
| CREATE VIEW  | 436 ms          | 228 ms          | 96 ms             | **78%** |
| DROP TABLE   | 311 ms          | 221 ms          | 97 ms             | **69%** |
| DROP VIEW    | —               | 211 ms          | 86 ms             | **59%** |

(Note: Session 11 is at ~28k objects vs 50k for earlier sessions. The remaining
O(n) scaling is very small, so the improvement would be slightly less at 50k.)

**Remaining optimization targets (from Session 10 profiling, re-ranked):**

Now that group_commit advancement is eliminated, the cost breakdown shifts.
The remaining O(n) costs (estimated) are:
1. **Transaction::new proto→Rust (~20-25%)** — Deserializes all 20 collections
2. **Snapshot clone (~20-25%)** — Clones all BTreeMaps from cached Snapshot
3. **apply_updates consolidation (~12-15%)** — O(n log n) sort of full trace
4. **Persist writes (~10-15%)** — compare_and_append, consolidation

**Next step:** Profile again to confirm the new cost breakdown, then tackle
Transaction::new lazy deserialization or Snapshot clone cost reduction.

---

## Session 12: Profile post-Session 11 optimized build

**Goal:** Profile the optimized build after eliminating the group_commit table
advancement loop (Session 11) to identify the next bottleneck and get an updated
cost breakdown.

**Setup:** Optimized build, ~28k objects, perf record at 99 Hz for 10 seconds.
30 CREATE TABLEs in 3.3s = ~111ms per DDL. 3068 samples captured.

**Top-level breakdown (% of active coordinator time):**

| Function                           | % of active | Est. time |
|------------------------------------|-------------|-----------|
| sequence_create_table (total)      | 80%         | ~89ms     |
| catalog_transact_with_side_effects | 70%         | ~78ms     |
| allocate_user_id                   | 8%          | ~9ms      |
| group_commit                       | 0%          | 0ms       |
| Non-DDL overhead                   | 20%         | ~22ms     |

**Detailed catalog_transact breakdown (% of catalog_transact time):**

| Function                             | % of ct | Est. time | Notes                              |
|--------------------------------------|---------|-----------|-------------------------------------|
| DurableCatalogState::transaction     | 38%     | ~30ms     | snapshot + Transaction::new         |
|   Snapshot::clone                    | 12%     | ~9ms      | items=8%, storage_coll_meta=3%      |
|   TableTransaction::new              | 26%     | ~20ms     | items=16%, storage_coll_meta=10%    |
| DurableCatalogState::commit_transaction | 28%  | ~22ms     | consolidate + compare_and_append    |
|   consolidate (inside apply_updates) | 24%     | ~19ms     | O(n log n) sort_unstable on trace   |
|   compare_and_append + pending       | 3%      | ~2ms      |                                     |
| apply_updates (standalone)           | 3%      | ~2ms      | CatalogState apply, excl consolidate |
| apply_catalog_implications           | 13%     | ~10ms     | create_table_collections, read holds |
| transact_op                          | 8%      | ~6ms      | user-provided DDL ops               |
| Transaction drop                     | 6%      | ~5ms      | BTreeMap destruction                 |

**Cross-cutting costs:**

- **BTreeMap clone/create/destroy** dominates the profile. The Snapshot uses
  `BTreeMap<ProtoKey, ProtoValue>` for each of ~20 collections, and
  TableTransaction converts these to `BTreeMap<RustKey, Option<RustValue>>`.
  The largest collections are `items` (~10k+ entries) and
  `storage_collection_metadata` (~10k+ entries).

- **Consolidation** (`consolidate_updates_slice_slow`) = 24% of ct. This sorts
  the entire StateUpdate trace using `sort_unstable` every DDL. The trace grows
  with object count making this O(n log n).

- **Proto-to-Rust conversion** in `TableTransaction::new` iterates each
  collection's BTreeMap, converts every entry from proto types to Rust types,
  and builds a new `BTreeMap<RustKey, Option<RustValue>>`. The `items` collection
  accounts for 16% of ct and `storage_collection_metadata` for 10%.

**Key findings vs Session 10:**

| Cost center              | Session 10 | Session 12 | Change          |
|--------------------------|-----------|-----------|-----------------|
| group_commit advancement | 23%       | 0%        | Eliminated (S11)|
| Transaction::new         | 20%       | 26%       | Now top cost    |
| Snapshot clone           | 19%       | 12%       | Relatively smaller|
| consolidate              | 12%       | 24%       | Now #2 cost     |
| apply_catalog_implications | 9%      | 13%       | Relatively larger|

The elimination of the 23% group_commit cost reshuffled the relative rankings.
Transaction::new (26%) and consolidation (24%) are now the top two bottlenecks.

**Optimization opportunities ranked by impact:**

1. **Make Transaction::new lazy (26% of ct, ~20ms).** Most DDL operations only
   touch 1-2 of the 20 collections. If `TableTransaction::new` is only called
   for collections that are actually accessed, the items and
   storage_collection_metadata conversions (26% total) would be avoided for most
   DDL types. This requires lazy initialization of each `TableTransaction`.

2. **Reduce consolidation cost (24% of ct, ~19ms).** The full trace is sorted
   on every DDL. Options:
   - Maintain the trace in sorted order (insert in sorted position)
   - Use incremental consolidation (only consolidate new entries)
   - Keep a separate sorted buffer and merge

3. **Reduce Snapshot clone cost (12% of ct, ~9ms).** Wrap the large BTreeMaps
   (`items`, `storage_collection_metadata`) in `Arc` so clone is O(1). The
   snapshot is cloned once per transaction and the clone is consumed by
   `Transaction::new`, so using `Arc` + clone-on-write would make the clone
   near-free.

4. **Reduce apply_catalog_implications (13% of ct, ~10ms).** This includes
   `create_table_collections` and read hold management. May be harder to
   optimize as it's real work for each DDL.

5. **Reduce Transaction drop cost (6% of ct, ~5ms).** The Transaction holds
   BTreeMaps for all 20 collections. If lazy init (#1) is implemented, only
   accessed collections would need to be dropped.

### Session 13: Incremental consolidation — replace O(n log n) sort with O(n) merge (2026-02-20)

**Goal:** Reduce the O(n log n) consolidation cost that was identified in Session
12 as 24% of catalog_transact time (~19ms at ~28k objects). Every DDL commit
calls `consolidate()` which sorts the entire in-memory trace with
`sort_unstable` and deduplicates. With ~28k entries, this is an O(n log n) cost
on every DDL.

**Analysis:** The consolidation works on `PersistHandle.snapshot`, a
`Vec<(StateUpdateKind, Timestamp, Diff)>` that accumulates all catalog state.
After each DDL, 1-5 new entries are pushed to this trace, then `consolidate()`
is called which:
1. Unifies all timestamps to the latest (O(n))
2. Calls `differential_dataflow::consolidation::consolidate_updates()` which
   sorts the entire vec with `sort_unstable` (O(n log n)) and deduplicates

The key insight: the trace is already consolidated (sorted, deduplicated) before
new entries are pushed. Only the last 1-5 entries are unsorted. Sorting the
entire 28k-entry vec to handle 5 new entries is wasteful.

**Approach:** Added `consolidate_incremental(old_len)` method that takes
advantage of the already-consolidated prefix:
1. Records `old_len = self.snapshot.len()` before pushing new entries
2. After pushing, extracts and consolidates only the new entries (O(m log m)
   where m = 1-5, trivially fast)
3. Merges the sorted old entries with the sorted new entries in a single pass
   (O(n + m)), unifying timestamps and cancelling entries with matching keys
   and opposite diffs during the merge

This replaces the O(n log n) full sort with an O(n) merge. The full
`consolidate()` method is preserved for `current_snapshot()` and other callers
outside the DDL hot path.

**Also investigated: lazy Transaction::new.** Before implementing the
consolidation fix, investigated making `Transaction::new` lazy (deferring
proto-to-Rust deserialization of the 20 collections). Found that CREATE TABLE
accesses both of the two expensive collections: `items` (16% of catalog_transact)
and `storage_collection_metadata` (10%). It also reads from `databases`,
`schemas`, `roles`, and `introspection_sources` for OID allocation. Since the
two largest collections are both accessed for the most common DDL type, lazy
init would save very little for CREATE TABLE specifically. Deferred this in
favor of the consolidation fix which benefits all DDL types equally.

**Code changes:**
- `src/catalog/src/durable/persist.rs` — added `consolidate_incremental` method
  on `PersistHandle`, modified `apply_updates` to use it instead of
  `consolidate()`. The incremental method extracts new entries, consolidates them,
  then merges with the sorted old portion in a single O(n) pass.

**Results (optimized build, ~28k and ~35k user objects):**

Note: This session used Docker CockroachDB (previous sessions used native
CockroachDB), which adds ~30-40ms of I/O latency per DDL. Absolute numbers are
not directly comparable with Sessions 11-12; catalog_transact metrics and scaling
slopes are the meaningful comparisons.

Catalog_transact time (from Prometheus histogram, wall time):
- At ~28k objects: avg **116.7ms** (30 samples)
- At ~35k objects: avg **134.9ms** (14 samples)
- Scaling slope: **~2.6ms per 1000 objects** for catalog_transact

Wall-clock DDL latency (median of 14 samples, optimized build):

| DDL Type     | ~28k objects | ~35k objects |
|-------------|-------------|-------------|
| CREATE TABLE | 169 ms      | 192 ms      |
| CREATE VIEW  | 128 ms      | —           |
| DROP TABLE   | 128 ms      | —           |
| DROP VIEW    | 110 ms      | —           |

Raw data (sorted, 14 samples each, ~28k objects):
- CREATE TABLE: 159 160 161 164 166 167 168 170 172 173 174 193 199 199
- CREATE VIEW:  124 125 126 127 127 127 127 128 129 130 130 134 157 158
- DROP TABLE:   123 124 124 125 127 127 128 128 128 128 130 131 131 133
- DROP VIEW:    107 107 108 108 108 109 109 110 111 111 111 119 138 146

Raw data (sorted, 14 samples, ~35k objects):
- CREATE TABLE: 182 186 188 189 190 191 191 192 193 194 194 195 221 231

**Key observations:**

- **Absolute numbers are ~30-40ms higher than Sessions 11-12** due to Docker
  CockroachDB overhead. Session 11 used native CockroachDB with lower latency.

- **The consolidation change is architecturally correct.** Replaces O(n log n)
  sort with O(n) merge. The merge allocates a new Vec and copies all entries
  (single O(n) pass), compared to the old in-place sort_unstable (which, for
  nearly-sorted data, may have been close to O(n) already via pdqsort's
  adaptive behavior).

- **All DDL operations work correctly** at scale (28k-35k objects). CREATE,
  DROP, CREATE VIEW, DROP VIEW all succeed.

- **Scaling slope of ~2.6ms per 1000 objects** for catalog_transact is consistent
  with the remaining O(n) costs (Transaction::new, Snapshot clone, etc.)

**Remaining optimization targets (from Session 12 profiling, updated):**

1. **Make Transaction::new lazy (~26% of catalog_transact, ~20ms).** Still the
   largest single cost. However, for CREATE TABLE specifically, both `items`
   (16%) and `storage_collection_metadata` (10%) are accessed, limiting the
   savings. Could still help for CREATE VIEW (which may not access
   `storage_collection_metadata`), ALTER, and other DDL types that touch fewer
   collections. Also saves Transaction drop time (6%).

2. **Reduce Snapshot clone cost (~12% of catalog_transact, ~9ms).** Wrap large
   BTreeMaps in `Arc` for O(1) clone. Combined with lazy Transaction::new,
   could avoid cloning unaccessed collections entirely.

3. **Reduce apply_catalog_implications (~13% of catalog_transact, ~10ms).**
   Includes `create_table_collections` and read hold management.

4. **Reduce OID allocation cost.** During CREATE TABLE, `allocate_oids` scans
   `databases`, `schemas`, `roles`, `items`, and `introspection_sources` to
   collect all allocated OIDs. This triggers initialization of those collections
   in Transaction::new. Maintaining a cached set of allocated OIDs could avoid
   this scan and make more collections eligible for lazy initialization.

### Session 14: Profile post-Session 13 optimized build at ~36.5k objects (2026-02-20)

**Goal:** Get `perf` profiling working and produce a fresh post-Session 13 cost
breakdown at the current object count (~36.5k).

**Setup:** Optimized build, Docker CockroachDB (`cockroachdb/cockroach:v23.1.11`),
~36,490 total objects. Used `perf record -g -F 997 -p <pid>` to profile 100
CREATE TABLE statements, yielding 23,442 samples.

**Profiling notes:** The `perf` binary for the running kernel (6.14.0-1015-aws)
wasn't available, but the older `perf` at `/usr/lib/linux-tools/6.8.0-100-generic/perf`
works after lowering `perf_event_paranoid` to 1. Used `rustfilt` for symbol
demangling.

**Absolute timing:**

| Metric | Value |
|--------|-------|
| CREATE TABLE median (psql) | ~163ms |
| catalog_transact avg (Prometheus) | 110.9ms (11.09s / 100) |
| 99th percentile bucket | <128ms |

**Cost breakdown (children% of total CPU, 23k samples):**

```
sequence_create_table:                     61.3%  (~127ms)
├── catalog_transact_with_ddl_transaction:  53.5%  (~111ms)
│   ├── catalog_transact_inner:              46.8%
│   │   ├── Catalog::transact:                38.7%
│   │   │   ├── DurableCatalogState::transaction:  19.7%  (~41ms)
│   │   │   │   ├── Snapshot::clone:                5.7%  (~12ms)
│   │   │   │   └── Transaction::new:              14.0%  (~29ms)
│   │   │   │       └── items deserialization:      9.5%  (~20ms)
│   │   │   ├── commit_transaction:                16.2%  (~34ms)
│   │   │   │   └── consolidate_incremental:       15.0%  (~31ms)
│   │   │   └── Transaction::commit:               10.9%  (~23ms)
│   │   │       ├── into_parts:                     2.4%
│   │   │       └── items pending:                  1.8%
│   │   └── validate_resource_limits:            7.8%  (~16ms)
│   │       └── 7+ full catalog scans via is_*
│   └── apply_catalog_implications:            6.7%  (~14ms)
│       └── ReadHold::try_downgrade:           5.5%  (~11ms)
└── non-catalog-transact:                    7.8%  (~16ms)
```

**Top self-time functions:**

| Function | Self % |
|----------|--------|
| imbl OrdMap Cursor::seek_to_first | 3.6% |
| BTreeMap search_tree (GlobalId → SetValZST) | 1.5% |
| malloc | 1.1% |
| CatalogEntry::is_continual_task | 1.05% |
| sdallocx (jemalloc free) | 0.83% |
| items DedupSortedIter::next | 0.78% |
| BTreeMap search_tree (GlobalId → CollectionState) | 0.76% |
| mpsc::UnboundedSender::send (ChangeBatch) | 0.67% |
| CatalogEntry::is_connection | 0.65% |
| CatalogEntry::is_temporary | 0.63% |

**Key findings:**

1. **consolidate_incremental still 15% despite O(n) optimization.** The Session 13
   O(n) merge is working correctly, but O(n) at 36k entries still means allocating
   a new Vec and copying ~36k `(StateUpdateKind, Timestamp, Diff)` tuples every
   commit. The merge is dominated by the sheer data movement. Changing the snapshot
   from `Vec` to `BTreeMap<StateUpdateKind, Diff>` would make consolidation
   O(m log n) per commit (where m=1-5 new entries), eliminating this cost almost
   entirely.

2. **validate_resource_limits is 7.8% — a new top target.** This function calls
   7+ separate `user_*()` methods (user_tables, user_sources, user_sinks,
   user_materialized_views, user_connections, user_secrets, user_continual_tasks),
   each of which iterates ALL entries in the `imbl::OrdMap<CatalogItemId,
   CatalogEntry>` with `is_*` type checks. Combined, the `is_*` functions account
   for ~4.5% of self-time. Could be eliminated by maintaining cached resource
   counts that are updated incrementally during catalog transactions.

3. **Snapshot::clone at 5.7%.** The durable catalog's `Snapshot` struct (21
   BTreeMaps) is fully cloned for each transaction in
   `DurableCatalogState::transaction()`. The `items` BTreeMap clone alone is
   likely ~4% given items deserialization is 9.5%. Wrapping BTreeMaps in `Arc`
   would make clone O(1), with copy-on-write semantics in `Snapshot::apply_update`.

4. **Transaction::new (14%) + commit/drop (10.9%) = 25%.** Proto deserialization
   on creation and re-serialization + BTreeMap drop on commit are together the
   largest combined cost. Items collection alone accounts for 9.5% (new) + 1.8%
   (pending) + drop time.

5. **apply_catalog_implications at 6.7%.** Dominated by `ReadHold::try_downgrade`
   (5.5%) which iterates a `BTreeMap<GlobalId, ReadHold>` of all storage
   collections. This is real per-DDL work but could potentially be batched.

6. **OID allocation (allocate_oids + get_temporary_oids) at ~3.5%.** The
   `allocate_oids` path in Transaction scans items (0.94%) and the
   `get_temporary_oids` path in CatalogState scans schemas + items (1.6%).
   Total OID cost is lower than expected (~3.5% vs the Transaction::new cost
   of 14%), but it forces initialization of multiple collections in lazy
   Transaction scenarios.

**Comparison with Session 12 profile (both at ~28-36k objects, optimized):**

| Component | Session 12 (% of catalog_transact) | Session 14 (% of total) | Notes |
|-----------|-----------------------------------|------------------------|-------|
| Transaction::new | 26% | 14.0% | Similar; % basis differs |
| consolidation | 24% | 15.0% | Was O(n log n), now O(n) |
| Snapshot clone | 12% | 5.7% | Unchanged |
| apply_catalog_implications | 13% | 6.7% | Unchanged |
| validate_resource_limits | not measured | 7.8% | New finding |

**Optimization targets (ranked by estimated absolute savings):**

1. **Change snapshot trace from Vec to BTreeMap** — eliminate 15% (~31ms).
   Use `BTreeMap<StateUpdateKind, Diff>` with a separate "current timestamp"
   field. Consolidation becomes O(m log n) per commit instead of O(n). Building
   Snapshot from trace is still O(n) but is already cached.

2. **Cache validate_resource_limits counts** — eliminate 7.8% (~16ms). Maintain
   a `ResourceCounts` struct in CatalogState, updated incrementally when
   entries are added/removed. Avoids 7+ full catalog iterations per DDL.

3. **Arc-wrap Snapshot BTreeMaps** — eliminate 5.7% (~12ms). Make Snapshot::clone
   O(1) via Arc. Use Arc::make_mut for copy-on-write in apply_update.

4. **Lazy Transaction::new** — eliminate portion of 14% (~29ms). Only
   deserialize collections actually accessed. For CREATE TABLE, both `items`
   and `storage_collection_metadata` are needed (together ~12%), so savings
   would be ~2% from skipping the other 18 collections. More beneficial for
   other DDL types.

5. **Optimize ReadHold::try_downgrade** — reduce 5.5% (~11ms). Investigate
   whether the downgrade can be done more efficiently or batched.

### Session 15: BTreeMap snapshot trace — eliminate O(n) consolidation (2026-02-20)

**Goal:** Replace the `Vec<(T, Timestamp, Diff)>` snapshot trace with
`BTreeMap<T, Diff>` to eliminate the O(n) consolidation cost that was 15% of
total CPU time in Session 14's profiling.

**Analysis:** The `consolidate_incremental` method (Session 13) performed an
O(n) merge on every commit: allocating a new Vec, copying all ~36k entries,
and merging in 1-5 new entries. With a BTreeMap, consolidation becomes
O(m log n) per commit — each new entry does a single BTreeMap lookup/insert.

**Approach:** Changed the snapshot data structure in `PersistHandle`:

1. **Field change:** `snapshot: Vec<(T, Timestamp, Diff)>` →
   `snapshot: BTreeMap<T, Diff>` + `snapshot_ts: Timestamp`. The timestamp is
   unified across all entries (was already the case after consolidation), so
   it's stored once instead of per-entry.

2. **apply_updates:** Instead of `push` + `consolidate_incremental`, each
   update does a BTreeMap `entry()` lookup. For matching keys, diffs are
   combined; zero-diff entries are removed. This is O(log n) per update.

3. **consolidate():** Becomes a no-op — the BTreeMap is always consolidated.

4. **consolidate_incremental():** Removed entirely — no longer needed.

5. **with_trace():** Reconstructed from BTreeMap on the fly (only called by
   `get_next_id`, which is infrequent).

6. **with_snapshot(), snapshot_id_allocator(), current_snapshot():** Updated
   to iterate `(kind, diff)` pairs from BTreeMap instead of
   `(kind, ts, diff)` tuples from Vec.

7. **Initialization paths:** Updated `audit_log` partition to use
   `BTreeMap::retain()` instead of `Vec::partition()`. Updated struct
   initialization to use `BTreeMap::new()` + `Timestamp::minimum()`.

Both `StateUpdateKind` and `StateUpdateKindJson` already derive `Ord`
(required by `IntoStateUpdateKindJson` trait bound), so the BTreeMap
constraint is naturally satisfied.

**Code changes:**
- `src/catalog/src/durable/persist.rs` — main changes to PersistHandle
- `src/catalog/src/durable/upgrade.rs` — updated snapshot iteration

**Results (median of 14 samples, optimized build, ~36.5k objects, Docker CockroachDB):**

| DDL Type     | Session 14 Baseline | Session 15 BTreeMap | Improvement |
|-------------|--------------------|--------------------|-------------|
| CREATE TABLE | 163 ms             | 134 ms             | **18%**     |
| CREATE VIEW  | —                  | 100 ms             | —           |
| DROP TABLE   | —                  | 110 ms             | —           |
| DROP VIEW    | —                  | 96 ms              | —           |

Prometheus catalog_transact_with_ddl_transaction (CREATE TABLE only):
- Session 14: 110.9ms avg (100 ops)
- Session 15: 97.7ms avg (14 ops)
- **12% improvement** in catalog_transact

Raw data (sorted, 14 samples each):
- CREATE TABLE: 130 131 132 133 133 133 134 134 135 136 137 146 161 163
- CREATE VIEW:  95 96 97 97 97 98 99 100 100 100 101 102 106 119
- DROP TABLE:   105 106 107 108 108 109 110 110 111 111 111 111 129 133
- DROP VIEW:    94 94 95 95 96 96 96 97 97 97 99 99 106 107

**Key observations:**

- **CREATE TABLE improved 18%** from 163ms → 134ms median. The consolidation
  cost (~31ms estimated in Session 14) was essentially eliminated, replaced
  by O(m log n) BTreeMap operations (~microseconds for m=5 entries).

- **Despite having ~36.5k objects (same as Session 14), all DDL types are
  significantly faster.** Session 13 measured 169ms CREATE TABLE at ~28k
  objects. Session 15 at ~36.5k objects is 134ms — faster despite 30% more
  objects, confirming the scaling improvement.

- **The BTreeMap approach eliminates an entire O(n) bottleneck** from the
  commit path. The commit_transaction cost is now dominated by other work
  (persist sync, etc.) rather than data structure maintenance.

- **Cumulative improvement from Session 6 baseline:** 444ms → 134ms = **70%
  reduction** (or ~75% accounting for Docker CockroachDB overhead).

---

## Session 16: Lazy validate_resource_limits counting

**Date:** 2026-02-20
**Type:** Optimization
**Build:** Optimized (release + debuginfo), Docker CockroachDB
**Object count:** ~36,500

### Goal

Eliminate unnecessary catalog scans in `validate_resource_limits`. The Session 14
profiling showed this function consuming ~7.8% of total CPU (~16ms), calling 7+
`user_*()` methods that each iterate ALL entries in the catalog with `is_*` type
checks. For CREATE TABLE, only `user_tables().count()` is needed — all other
resource type deltas are 0.

### Change

Instead of caching counts (which requires maintaining a `ResourceCounts` struct
through all catalog mutations), used a simpler approach: guard each expensive
counting operation with a check on whether the corresponding `new_*` delta is
positive. Since `validate_resource_limit` already short-circuits when
`new_instances <= 0` (returning `Ok(())` without using the `current_amount`),
passing `0` when no new instances are being created is semantically equivalent.

For CREATE TABLE, this eliminates ~10 unnecessary catalog scans (connections,
sources, sinks, materialized views, clusters, databases, secrets, roles,
continual tasks, network policies), leaving only `user_tables().count()` and
the per-schema items count.

**File changed:** `src/adapter/src/coord/ddl.rs`

### Results

| DDL Type     | Session 15 median  | Session 16 median  | Improvement |
|--------------|--------------------|--------------------|-------------|
| CREATE TABLE | 134 ms             | 127 ms             | -5%         |
| CREATE VIEW  | 100 ms             | 92 ms              | -8%         |
| DROP TABLE   | 110 ms             | 100 ms             | -9%         |
| DROP VIEW    | 96 ms              | 87 ms              | -9%         |

Prometheus catalog_transact_with_ddl_transaction (CREATE TABLE only):
- Session 15: 97.7ms avg (14 ops)
- Session 16: 90.2ms avg (100 ops)
- **8% improvement** in catalog_transact

Raw data (sorted, 100 CREATE TABLE samples):
121 122 122 122 123 123 123 123 123 123 124 124 124 124 124 124 124 124 124 124
125 125 125 125 125 125 125 125 125 125 125 125 125 126 126 126 126 126 126 126
126 126 127 127 127 127 127 127 127 127 127 127 127 127 127 127 128 128 128 128
128 128 128 128 128 128 128 128 128 129 129 129 129 129 129 129 129 129 130 130
130 130 131 131 132 134 134 139 146 151 155 156 156 156 157 157 158 158 158 158

**Key observations:**

- **CREATE TABLE improved 5%** from 134ms → 127ms median. The improvement is
  modest because CREATE TABLE only has one positive delta (new_tables=1), so we
  skip ~10 O(n) scans but still pay for `user_tables().count()` and other
  fixed costs.

- **DROP VIEW improved most (9%)** — drops have zero positive deltas for all
  resource types, so the entire `validate_resource_limits` counting section is
  skipped entirely.

- **The improvement was larger than expected.** Session 14 profiling estimated
  7.8% CPU on validate_resource_limits, but the actual wall-clock improvement
  suggests the catalog scans had additional cache/memory effects beyond pure
  CPU cost.

- **Cumulative improvement from Session 6 baseline:** 444ms → 127ms = **71%
  reduction** (or ~76% accounting for Docker CockroachDB overhead).

### Session 17: Wall-clock profiling — DDL bottleneck shifted from CPU to I/O (2026-02-20)

**Goal:** Profile the optimized build after all Sessions 7-16 optimizations to
get an updated cost breakdown and identify the next bottleneck.

**Setup:** Optimized build, Docker CockroachDB, ~37k objects. Used Prometheus
metrics (histogram deltas before/after 100-DDL batches) for wall-clock breakdown
since CPU profiling (`perf record`) showed only 0.6ms CPU per DDL — the process
is now almost entirely I/O bound.

**Key finding: CPU profiling is no longer useful.** `perf stat` measured only
60ms of total CPU time for 100 CREATE TABLE statements (0.6ms CPU per DDL).
Our CPU optimizations (Sessions 7-16) have reduced CPU work per DDL to
negligible levels. The remaining ~106ms of wall-clock time is dominated by
async I/O waits: persist operations, timestamp oracle round-trips, storage
controller IPC, and group commit.

**Absolute timing (100 CREATE TABLE batch, median ~37k objects):**

| Metric | Value |
|--------|-------|
| Wall-clock per DDL (batch) | 105.6 ms |
| Wall-clock per DDL (individual psql) | 128 ms |
| catalog_transact avg (Prometheus) | 89.1 ms |
| apply_catalog_implications avg | 42.0 ms |
| allocate_id avg | 3.2 ms |
| group_commit_initiate avg | 6.7 ms |
| catalog commit_latency avg | 5.8 ms per commit |

**Wall-clock breakdown per CREATE TABLE (~106ms batch mode):**

```
Total DDL: ~106ms
├── catalog_transact_with_ddl_transaction: 89.1ms (84%)
│   ├── apply_catalog_implications: 42.0ms (40%)       ← NEW top cost!
│   │   ├── get_local_write_ts (oracle): ~2.2ms
│   │   ├── confirm_leadership (catalog): ~1ms
│   │   ├── create_collections (storage ctrl): ~20-25ms ← dominant I/O
│   │   ├── apply_local_write (oracle): ~2.2ms
│   │   └── initialize_read_policies: ~10ms
│   │       ├── oracle.read_ts(): ~0.5ms
│   │       └── read hold setup + IPC: ~9ms
│   └── Remaining catalog_transact: 47.1ms (45%)
│       ├── DurableCatalogState::transaction: ~12-15ms (CPU: Snapshot clone + Transaction::new)
│       ├── Transaction::commit (in-memory): ~3-5ms (CPU)
│       ├── commit_transaction (persist write): ~3ms (I/O)
│       ├── validate_resource_limits: ~2ms (CPU, mostly skipped after S16)
│       └── transact_op + overhead: ~20ms (mixed)
├── allocate_id: 3.2ms (3%)
└── Non-catalog overhead: 13.3ms (13%)
    ├── group_commit: 6.7ms
    └── pgwire/connection: ~7ms
```

**Per-operation I/O latencies (from Prometheus):**

| Operation | Avg latency | Count | Notes |
|-----------|------------|-------|-------|
| oracle write_ts | 2.23 ms | 2686 | CockroachDB |
| oracle apply_write | 2.24 ms | 1652 | CockroachDB |
| oracle read_ts | 0.48 ms | 1861 | CockroachDB |
| persist compare_and_append | 2.88 ms | 20551 | Blob store |
| catalog commit | 2.80 ms | 1019 | compare_and_append |

**Key finding: apply_catalog_implications is 42ms = 47% of catalog_transact.**
In Session 14's CPU profiling, it was only 6.7% because CPU profiling misses
I/O waits. The wall-clock time is dominated by async I/O operations in the
`create_table_collections` path:

1. **Storage controller create_collections (~20-25ms):** Opens a persist
   WriteHandle for the table's data shard and registers it with txn-wal. Each
   of these involves a `compare_and_append` operation (~3ms each) plus async
   channel round-trip overhead between coordinator and storage controller.

2. **Timestamp oracle operations (~5ms):** `get_local_write_ts()` (2.2ms) +
   `apply_local_write()` (2.2ms) involve CockroachDB queries via the PostgreSQL
   timestamp oracle.

3. **Initialize read policies (~10ms):** oracle.read_ts() + read hold
   acquisition and downgrade, which sends IPC messages to storage controller.

**Comparison with Session 14 (CPU profile vs wall-clock):**

| Component | Session 14 (CPU %) | Session 17 (wall-clock ms) | Notes |
|-----------|-------------------|---------------------------|-------|
| DurableCatalogState::transaction | 19.7% | ~12-15ms | CPU-bound, correctly measured |
| consolidation | 15.0% | ~0ms | Eliminated in Session 15 |
| Transaction::new | 14.0% | included above | CPU-bound |
| Transaction::commit | 10.9% | ~3-5ms | CPU-bound, Transaction drop smaller |
| validate_resource_limits | 7.8% | ~2ms | Mostly eliminated in Session 16 |
| apply_catalog_implications | 6.7% | **42.0ms** | **6x larger in wall-clock!** |
| commit_transaction | 16.2% | ~3ms | Consolidation eliminated |

**Why CPU profiling showed the wrong picture:** CPU profiling (perf, flamegraphs)
only captures compute time. It misses async I/O waits where the coordinator yields
to the tokio runtime. This is why apply_catalog_implications appeared to be only
6.7% in Session 14 but is actually 42ms = 40% of wall-clock time. Future
profiling should use Prometheus wall-clock histograms, not CPU sampling.

**Optimization targets (ranked by wall-clock impact):**

1. **Reduce create_collections latency (~20-25ms, 19-24% of DDL).** The
   storage controller opens persist handles and registers with txn-wal. Could
   potentially:
   - Fire-and-forget the collection creation (don't wait for completion)
   - Overlap with other async operations (oracle, read policy init)
   - Pre-open persist handles lazily in the background

2. **Overlap apply_catalog_implications with group_commit (~7ms).** Currently
   sequential. If the group_commit could start before apply_catalog_implications
   finishes (or vice versa), saves ~7ms.

3. **Reduce initialize_read_policies latency (~10ms).** Read hold acquisition
   and downgrade involves IPC to the storage controller. Could batch or defer.

4. **Merge allocate_id into catalog_transact (~3ms).** Saves one persist
   round-trip by allocating IDs within the main transaction.

5. **Reduce DurableCatalogState::transaction CPU cost (~12-15ms).** This is
   Snapshot clone + Transaction::new (proto→Rust conversion). Arc-wrapping
   Snapshot BTreeMaps would save ~2-3ms of clone overhead. Lazy Transaction::new
   saves little for CREATE TABLE (accesses most collections).

**Summary:** The DDL optimization has reached a phase transition. After Sessions
7-16 eliminated all major CPU bottlenecks, DDL latency is now dominated by
sequential async I/O operations. Further improvements require architectural
changes: parallelizing I/O, reducing round-trips, or making I/O operations
non-blocking. The CPU work per DDL (~0.6ms) is now negligible.

### Session 18: Eliminate redundant WriteHandle in storage_collections for tables (2026-02-20)

**Goal:** Reduce `create_collections` latency (~20-25ms, 19-24% of DDL) by
eliminating a redundant persist WriteHandle open in
`StorageCollectionsImpl::create_collections_for_bootstrap()`.

**Analysis:** During CREATE TABLE, two separate layers open persist WriteHandles
for the same data shard:

1. **`StorageCollectionsImpl::create_collections_for_bootstrap()`** opens
   `WriteHandle` + `SinceHandle` for every collection via `open_data_handles()`.
   For tables, the WriteHandle is only used to extract `shard_id()` (already
   known from metadata) and `upper()` (overwritten to `initial_txn_upper` for
   txn-managed tables). The WriteHandle is then sent to the BackgroundTask where
   it's immediately dropped for `is_in_txns` collections.

2. **`StorageController::create_collections_for_bootstrap()`** opens a separate
   `WriteHandle` for each collection via its own `open_data_handles()`. For
   tables, this handle is used for txn-wal registration via
   `persist_table_worker.register()`.

Each redundant WriteHandle involves `open_writer()` + `fetch_recent_upper()` —
two sequential persist operations.

**Change:** Added `open_since_handle()` method to `StorageCollectionsImpl` that
opens only the SinceHandle (including `upgrade_version` and
`open_critical_handle`) without opening a WriteHandle. For tables
(`DataSource::Table`), the `create_collections_for_bootstrap` method now uses
`open_since_handle` instead of `open_data_handles`, and uses `initial_txn_upper`
directly as the `write_frontier` (since it would be overwritten to that value
anyway). The `register_handles` method was updated to accept
`Option<WriteHandle>` and a separate `shard_id` parameter.

**Code changes:**
- `src/storage-client/src/storage_collections.rs` — added `open_since_handle()`
  method, modified `create_collections_for_bootstrap()` to skip WriteHandle for
  tables, updated `register_handles()` and `BackgroundCmd::Register` to handle
  `Option<WriteHandle>` with explicit `shard_id`.

**Results (optimized build, Docker CockroachDB, ~37.5k objects):**

A/B test comparing baseline (pre-optimization) vs optimized build at the same
object count:

| Metric | Baseline (avg) | Optimized (avg) | Change |
|--------|---------------|----------------|--------|
| Batch per DDL | 110-111ms | 113-114ms | ~0 (noise) |
| catalog_transact | 94.8ms | 98.0ms | ~0 (noise) |
| apply_catalog_implications | 46.7ms | 47.1ms | ~0 (noise) |
| allocate_id | 3.5ms | 3.5ms | ~0 |

**No measurable improvement with local filesystem persist.** The eliminated
`open_writer()` + `fetch_recent_upper()` operations take ~0.5-1ms each on local
disk, which is within the measurement noise floor.

**Key insight: persist blob location matters.** This test uses local filesystem
persist (`file:///...`), where each persist operation takes ~0.5ms. In
production with S3-backed persist, each operation takes ~10-50ms. The redundant
WriteHandle elimination would save ~20-50ms per DDL in production — a
significant improvement.

**The change is architecturally correct** and eliminates genuinely redundant
work. Even though it doesn't measurably help in the test setup:
- Tables' WriteHandle in storage_collections is opened, used only for metadata
  extraction, then dropped — pure waste
- The storage controller opens its own WriteHandle for the same shard
- For txn-managed tables, the write_frontier from the WriteHandle is
  immediately overwritten to `initial_txn_upper`

**Updated wall-clock baseline at ~37.5k objects (Docker CockroachDB):**

| Metric | Session 17 (~37k) | Session 18 (~37.5k) | Notes |
|--------|-------------------|---------------------|-------|
| catalog_transact | 89.1ms | ~95ms | +6ms, mostly from more objects |
| apply_catalog_implications | 42.0ms | ~47ms | +5ms, mostly from more objects |
| batch per DDL | 105.6ms | ~112ms | +6ms |

The ~6ms increase from Session 17 is consistent with ~500 more objects at
~2.6ms/1000 objects scaling slope (~1.3ms) plus normal Docker CockroachDB
variance.

**Remaining optimization targets (same as Session 17, with updated notes):**

1. **Reduce `create_collections` latency — focus on production-impactful
   operations.** The storage_collections still opens `upgrade_version` +
   `open_critical_handle` + `compare_and_downgrade_since` sequentially for each
   table. These could potentially be overlapped with the storage_controller's
   `open_writer` + `fetch_recent_upper`. Requires restructuring to return the
   SinceHandle from storage_collections and pass it to the storage_controller,
   or to pipeline the operations.

2. **Overlap `apply_catalog_implications` with `group_commit`.** Already
   partially overlapped: `group_commit()` spawns a background task before
   `apply_catalog_implications` starts. The `table_updates` future (waiting for
   the background task) may already resolve during `apply_catalog_implications`.
   Net savings would be small.

3. **Reduce `initialize_read_policies` latency (~10ms).** Oracle `read_ts()` +
   read hold acquisition/downgrade.

4. **Merge `allocate_id` into `catalog_transact` (~3ms).** Saves one persist
   round-trip.

5. **Reduce `DurableCatalogState::transaction` CPU cost (~12-15ms).**
   Arc-wrapping Snapshot BTreeMaps, lazy Transaction::new.

---

## Session 19: Wall-clock profiling of create_collections breakdown

**Goal:** Instrument and measure the internal wall-clock breakdown of
`create_collections` — identified in Session 17 as costing 20-25ms (19-24% of
DDL). Understand where time goes within that call to target optimizations.

**Approach:** Added Prometheus histogram metrics at two levels:
1. **Adapter level** (`mz_create_table_collections_seconds{step=...}`):
   `get_local_write_ts`, `confirm_leadership`, `create_collections`,
   `apply_local_write`
2. **Storage controller level** (`mz_storage_create_collections_seconds{step=...}`):
   `storage_collections`, `open_data_handles`, `sequential_loop`, `table_register`
3. **Adapter level** (`mz_initialize_storage_collections_seconds`): read policy
   initialization

Also ran measurements in BOTH debug and optimized builds to verify proportions.

### Results: Optimized build (~37.5k objects, Docker CockroachDB)

**Wall-clock total:** 136ms per individual psql CREATE TABLE

**Full DDL breakdown (per-DDL averages, 20 DDL batch):**

```
catalog_transact_with_side_effects: 95ms
├── catalog_transact_inner: ~49ms
└── apply_catalog_implications: ~46ms
    ├── create_table_collections: ~38ms
    │   ├── get_local_write_ts:    2.0ms ( 5%)
    │   ├── confirm_leadership:    0.7ms ( 2%)
    │   ├── create_collections:   33.0ms (87%)    ← DOMINANT
    │   │   ├── storage_collections: ~24ms (73%)  ← persist handle opens
    │   │   ├── open_data_handles:    0.7ms ( 2%) ← write handle for txn-wal
    │   │   ├── sequential_loop:      0.1ms ( 0%) ← CPU work
    │   │   └── table_register:       6.5ms (20%) ← txn-wal registration
    │   └── apply_local_write:     2.3ms ( 6%)
    ├── initialize_storage_collections: 0.5ms
    └── overhead (classification):      ~7.5ms
```

**storage_collections (24ms) breakdown — three sequential persist operations:**
1. `upgrade_version` (~8ms) — schema compatibility check
2. `open_critical_handle` (~8ms) — CriticalSince handle
3. `compare_and_downgrade_since` (~8ms) — advance since for new tables

### Results: Debug build (~37.5k objects)

**Wall-clock total:** 3340ms per individual psql CREATE TABLE

**Key proportional differences from optimized:**

| Component | Debug | Optimized |
|-----------|-------|-----------|
| storage_collections | 2.5% | **73%** |
| table_register | **95%** | 20% |

The debug build massively inflates CPU-heavy operations (txn-wal serialization,
txn cache management) while I/O operations stay similar. This makes debug-build
profiling **misleading for I/O-bound operations** — always verify with optimized.

### Key insights

1. **`storage_collections` (since handle opens) is the dominant cost at 73% of
   `create_collections` in optimized builds.** It performs three sequential
   persist operations (~8ms each) for every new table: upgrade_version,
   open_critical_handle, compare_and_downgrade_since.

2. **`table_register` (txn-wal registration) is second at 20% (~6.5ms).** This
   involves try_register_schema, txns_cache.update_ge, compare-and-append to
   txns shard, and apply_le.

3. **Debug vs optimized proportions are completely different.** In debug,
   table_register appears to be 95% of cost (due to amplified CPU work in
   persist serialization). In optimized, it's only 20%. This confirms Session
   17's warning: always use Prometheus metrics on optimized builds for accurate
   proportions.

4. **`initialize_storage_collections` is negligible at 0.5ms** — much less than
   Session 17's estimate of ~10ms. This may have been reduced by prior
   optimizations or was overestimated.

5. **`get_local_write_ts` + `confirm_leadership` = 2.7ms** — relatively small
   compared to create_collections.

### Optimization targets (updated ranking by optimized-build wall-clock)

1. **Reduce storage_collections persist operations (~24ms, 73% of
   create_collections).** Three sequential persist ops for every table:
   upgrade_version, open_critical_handle, compare_and_downgrade_since.
   Potential: overlap with open_data_handles/table_register, eliminate
   upgrade_version for new shards, or defer compare_and_downgrade_since.

2. **Reduce table_register latency (~6.5ms, 20% of create_collections).**
   txn-wal registration involves multiple persist operations. Could overlap
   with post-create_collections work (apply_local_write, read policy init).

3. **Overlap storage_collections and open_data_handles.** Currently sequential.
   The write handle open doesn't depend on the since handle (only
   fetch_recent_upper does, and tables don't use it). Could save ~0.7ms by
   overlapping them, or more if we also overlap table_register.

### Changes made

- Added `mz_create_table_collections_seconds` histogram (adapter metrics)
- Added `mz_initialize_storage_collections_seconds` histogram (adapter metrics)
- Added `mz_storage_create_collections_seconds` histogram (storage controller)
- No functional changes — diagnostic only

## Session 20: Skip upgrade_version CAS for shards at current version

**Date:** 2026-02-20
**Type:** Optimization
**Build:** Optimized (release + debuginfo), Docker CockroachDB
**Object count:** ~37.5k → ~37.9k

### Goal

Reduce `storage_collections` persist operations (~24ms, 73% of create_collections
in Session 19). Three sequential persist CAS operations happen for every new table:
`upgrade_version`, `open_critical_handle` (includes `register_critical_reader` +
`compare_and_downgrade_since`), and a table-specific `compare_and_downgrade_since`.

### Analysis

For newly-created shards, `upgrade_version` is a no-op: the shard is initialized
with `cfg.build_version` (the current version) via `maybe_init_shard` →
`write_initial_rollup` → `TypedState::new(cfg.build_version, ...)`. The
`upgrade_version` work function checks `state.version <= cfg.build_version` and
sets `state.version = cfg.build_version` — but for a new shard, the version is
already equal. Despite this, the code returns `Continue(Ok(()))` which triggers
a consensus CAS write that changes nothing.

This is also true for existing shards after restart without a version change: the
shard version matches the build version, but `upgrade_version` still performs a
consensus CAS write.

### Change

Modified `Machine::upgrade_version()` in `src/persist-client/src/internal/machine.rs`
to distinguish three cases:
1. `state.version == cfg.build_version` → `Break(NoOpStateTransition(Ok(())))`:
   Already at current version, skip consensus CAS write entirely.
2. `state.version < cfg.build_version` → `Continue(Ok(()))`: Needs upgrade,
   perform CAS write as before.
3. `state.version > cfg.build_version` → `Break(NoOpStateTransition(Err(...)))`:
   Incompatible version, return error as before.

When `Break(NoOpStateTransition(...))` is returned, the persist state machine
skips the `try_compare_and_set_current` consensus write entirely via the
`ApplyCmdResult::SkippedStateTransition` path. This saves one consensus round-trip
(~3-4ms with local filesystem persist, ~10-50ms with S3-backed persist in
production).

**Benefits beyond DDL:**
- Bootstrap: All shards already at current version skip the CAS during restart
  (~thousands of shards × ~4ms each = significant startup time savings)
- Any other caller of `upgrade_version` benefits from the same optimization

### Results (optimized build, Docker CockroachDB, ~37.5-37.9k objects)

**100-batch averages (most reliable due to variance smoothing):**

| Metric | Session 19 (20 batch) | Session 20 (100 batch) | Change |
|--------|----------------------|-----------------------|--------|
| catalog_transact avg | ~95ms | 90.4ms | **-5%** |
| apply_catalog_implications avg | ~46ms | 41.5ms | **-10%** |
| storage_collections avg | ~24ms | 20.9ms | **-13%** |
| table_register avg | ~6.5ms | 8.2ms | +26% (noise) |
| create_collections avg | ~33ms | 29.9ms | **-9%** |
| allocate_id avg | ~3.5ms | 3.5ms | ~0 |

**Individual psql measurements (14 samples, sorted):**
122 124 125 126 127 127 127 127 127 129 129 137 147 156
Median: **127ms** (Session 19: 136ms, **-7%**)

**Batch mode throughput:**
- Batch of 100: 105ms per DDL (Session 19: ~105ms batch mode)

**Note on variance:** Docker CockroachDB shows significant batch-to-batch variance.
20-batch measurements ranged from 75ms to 89ms for catalog_transact. The 100-batch
average (90.4ms) is the most reliable comparison point.

### Key observations

1. **storage_collections dropped ~3ms** from ~24ms to ~20.9ms. This matches the
   expected savings from skipping one consensus CAS write (~3-4ms with local
   filesystem persist).

2. **Individual psql median dropped 9ms** from 136ms to 127ms. Part of the
   improvement is from the storage_collections savings; the rest may be from
   reduced contention (one fewer CAS means less consensus load).

3. **table_register variance is high** (5.4-8.2ms across batches). This is
   Docker CockroachDB variance, not a regression.

4. **The optimization benefits all shards, not just DDL.** During bootstrap,
   thousands of existing shards that are already at the current version will
   skip the consensus CAS. This could save significant startup time.

### Updated wall-clock breakdown per CREATE TABLE (~127ms individual psql)

```
Total DDL: ~127ms
├── catalog_transact: 90ms (71%)
│   ├── apply_catalog_implications: 42ms (33%)
│   │   ├── get_local_write_ts: ~2.1ms
│   │   ├── confirm_leadership: ~0.8ms
│   │   ├── create_collections: ~30ms                    ← was ~33ms
│   │   │   ├── storage_collections: ~21ms (was ~24ms)   ← IMPROVED
│   │   │   │   ├── open_critical_handle: ~13ms (2 CAS)
│   │   │   │   └── upgrade_version: ~8ms (make_machine + no-op CAS skip)
│   │   │   ├── open_data_handles: ~0.7ms
│   │   │   └── table_register: ~8ms
│   │   └── apply_local_write: ~2.3ms
│   └── catalog_transact_inner: ~48ms
├── allocate_id: 3.5ms (3%)
└── Non-catalog overhead: ~34ms (27%)
```

### Remaining optimization targets (updated)

1. **Overlap storage_collections with open_data_handles + table_register
   (~8ms savings).** Currently sequential: storage_collections (21ms) must
   complete before the controller opens WriteHandles (0.7ms) and does
   table_register (8ms). Since the WriteHandle and table_register don't
   depend on the SinceHandle, they could run concurrently with
   storage_collections. Saves up to 8ms (table_register running during the
   21ms storage_collections window).

2. **Combine compare_and_downgrade_since calls (~4ms savings).** For new tables,
   `open_critical_handle` sets the since to T::minimum() and then a separate CAS
   advances it to register_ts. Could pass register_ts directly to
   `open_critical_handle`. Challenge: during bootstrap, existing tables' since
   must NOT be advanced to register_ts. Needs a way to distinguish DDL from
   bootstrap (e.g., a parameter to create_collections_for_bootstrap).

3. **Merge allocate_id into catalog_transact (~3ms).** Saves one persist
   round-trip.

4. **Reduce DurableCatalogState::transaction CPU cost (~12-15ms).** Arc-wrapping
   Snapshot BTreeMaps, lazy Transaction::new.
